hdfs dfs -mkdir -p /data/myfiles
hdfs dfs -put name.csv /data/myfiles/
hdfs dfs -ls  /data/myfiles
hive
set hive.execution.engine=mr;
set hive.execution.engine=spark;

CREATE TABLE my_table (
  id INT,
  name STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA INPATH '/data/myfiles/data.csv' INTO TABLE my_table;

set hive.execution.engine=spark;
set hive.execution.engine=spark;set spark.master=yarn;set spark.yarn.jars=hdfs://master/spark/jars/*.jar;
hive -e "set hive.execution.engine=spark;set spark.master=yarn;set spark.yarn.jars=hdfs://master/spark/jars/*.jar;;SELECT * FROM my_table;"

beeline -u "jdbc:hive2://" -n jupyter -p jupyter -e "set hive.execution.engine=mr;set mapreduce.framework.name=yarn;SELECT * FROM my_table;"